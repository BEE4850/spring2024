---
title: "Exploratory Data Analysis, Correlations, and Autocorrelations"
subtitle: "Lecture 05"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 5, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
engine: julia
julia:
    exeflags: ["+1.10.4"]          
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using Plots
using GLM
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using CSV
using DataFrames
using Optim

Random.seed!(1)
```

```{julia}
#| output: false
#| echo: false

# use default values of S=3.2°C, d=100m, α=1.3
function ebm(rf_nonaerosol, rf_aerosol; p=(3.2, 100, 1.3))
    # set up model parameters
    S, d, α = p # this unpacks the parameter tuple into variables
    F2xCO₂ = 4.0 # radiative forcing [W/m²] for a doubling of CO₂
    λ = F2xCO₂ / S

    c = 4.184e6 # heat capacity/area [J/K/m²]
    C = c*d # heat capacity of mixed layer (per area)
    F = rf_nonaerosol + α*rf_aerosol # radiative forcing
    Δt = 31558152. # annual timestep [s]

    T = zero(F)
    for i in 1:length(F)-1
        T[i+1] = T[i] + (F[i] - λ*T[i])/C * Δt
    end
    
    # return after normalizing to reference period
    return T .- mean(T[1:20])
end

# Dataset from https://zenodo.org/record/3973015
# The CSV is read into a DataFrame object, and we specify that it is comma delimited
forcings_all_85 = CSV.read("data/climate/ERF_ssp585_1750-2500.csv", DataFrame, delim=",")

# Separate out the individual components
forcing_co2_85 = forcings_all_85[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_85 = forcings_all_85[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_85 = forcings_all_85[!,"aerosol-cloud_interactions"]
forcing_aerosol_85 = forcing_aerosol_rad_85 + forcing_aerosol_cloud_85
forcing_total_85 = forcings_all_85[!,"total"]
forcing_non_aerosol_85 = forcing_total_85 - forcing_aerosol_85
forcing_other_85 = forcing_total_85 - (forcing_co2_85 + forcing_aerosol_85)

forcings_all_26 = CSV.read("data/climate/ERF_ssp126_1750-2500.csv", DataFrame, delim=",")

# Separate out the individual components
forcing_co2_26 = forcings_all_26[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_26 = forcings_all_26[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_26 = forcings_all_26[!,"aerosol-cloud_interactions"]
forcing_aerosol_26 = forcing_aerosol_rad_26 + forcing_aerosol_cloud_26
forcing_total_26 = forcings_all_26[!,"total"]
forcing_non_aerosol_26 = forcing_total_26 - forcing_aerosol_26
forcing_other_26 = forcing_total_26 - (forcing_co2_26 + forcing_aerosol_26)

t = time_forcing = Int64.(forcings_all_85[!,"year"]) # Ensure that years are interpreted as integers
sim_years = 1850:2100 # model years for projections
sim_idx = indexin(sim_years, t)

temps = CSV.read("data/climate/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv", DataFrame, delim=",")

time_obs = temps[:, 1]
temp_obs = temps[:, 2]
temp_lo = temps[:, 3]
temp_hi = temps[:, 4]

# generate simulations
hind_years = 1850:2020 # model years to simulate for fitting
sim_years = 1850:2100 # model years for projections
idx = indexin(hind_years, t) # find indices in t vector of simulation years
# since we specified default values for p, those are used for the parameters
temp_default = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx]) 

temp_obs = temp_obs[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_lo = temp_lo[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_hi = temp_hi[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_obs = temp_obs .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_lo = temp_lo .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_hi = temp_hi .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
```

# Last Class(es)

## Probability Models

**Goal**: Write down a probability model for the data-generating process for $\mathbf{y}$.

- Direct statistical model, $$\mathbf{y} \sim \mathcal{D}(\theta).$$
- Model for the residuals of a numerical model, $$\mathbf{r} = \mathbf{y} - F(\mathbf{x}) \sim \mathcal{D}(\theta).$$

## Model Fitting as Maximum Likelihood Estimation

We can interpret fitting a model (reducing error according to some loss or error metric) as maximizing the probability of observing our data from this data generating process.

# Exploratory Data Analysis (EDA)

## EDA: Assessing Model-Data Fit

**Examples**:

::: {.fragment .fade-in}

- Skewness
- Tails
- Clusters
- (Auto)correlations
:::

## Curve Fitting

:::: {.columns}
::: {.column width=70%}
![](https://imgs.xkcd.com/comics/curve_fitting_2x.png){width=40%}
:::
::: {.column width=30%}
::: {.caption}
Source: [XKCD #2048](https://xkcd.com/2048/)
:::
:::
::::

## Is EDA Ever Model-Free?

Some characterize EDA as model-free (versus "confirmatory" data analysis).

Is this right?

## Visual Approaches to EDA

Make plots!

- Histograms
- Scatterplots/Pairs plots
- Quantile-Quantile Plots

## Skew

```{julia}
#| label: fig-skew
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Skew"
#| fig-subcap: 
#|  - "Normal vs. Skewed Data"
#|  - "Histograms"

# specify regression model
f(x) = 2 + 2 * x
pred = rand(Uniform(0, 10), 20)
trend = f.(pred)

# generate normal residuals
normal_residuals = rand(Normal(0, 3), length(pred))
normal_obs = trend .+ normal_residuals

## generate skewed residuals
skewed_residuals = rand(SkewNormal(0, 3, 2), length(pred))
skewed_obs = trend .+ skewed_residuals

# make plots
# scatterplot of observations
p1 = plot(0:10, f.(0:10), color=:black, label="Trend Line", linewidth=2, guidefontsize=18, tickfontsize=16, legendfontsize=16) # trend line
scatter!(p1, pred, normal_obs, color=:orange, markershape=:circle, label="Normal Residuals")
scatter!(p1, pred, skewed_obs, color=:green, markershape=:square, label="Skewed Residuals")
xlabel!(p1, "Predictors")
ylabel!(p1, "Observations")
plot!(p1, size=(600, 450))

# densities of residual distributions
p2 = histogram(rand(Normal(0, 3), 1000), color=:orange, alpha=0.5, label="Normal Distribution", guidefontsize=18, tickfontsize=16, legendfontsize=16)
histogram!(p2, rand(SkewNormal(0, 3, 2), 1000), color=:green, alpha=0.5, label="SkewNormal Distribution")
xlabel!("Value")
ylabel!("Count")
plot!(p2, size=(600, 450))

display(p1)
display(p2)
```

## Fat Tails

```{julia}
#| label: fig-cauchy
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Fat Tails"
#| fig-subcap: 
#|  - "Normal vs. Fat-Tailed Data"
#|  - "Histograms"

# generate normal residuals
normal_residuals = rand(Normal(0, 3), length(pred))
normal_obs = trend .+ normal_residuals

## generate fat-tailed residuals
cauchy_residuals = rand(Cauchy(0, 1), length(pred))
cauchy_obs = trend .+ cauchy_residuals

# make plots
# scatterplot of observations
p1 = plot(0:10, f.(0:10), color=:black, label="Trend Line", linewidth=2, guidefontsize=18, tickfontsize=16, legendfontsize=16) # trend line
scatter!(p1, pred, normal_obs, color=:orange, markershape=:circle, label="Normal Residuals")
scatter!(p1, pred, cauchy_obs, color=:green, markershape=:square, label="Fat-Tailed Residuals")
xlabel!(p1, "Predictors")
ylabel!(p1, "Observations")
plot!(p1, size=(600, 450))

# densities of residual distributions
p2 = histogram(rand(Normal(0, 3), 1000), color=:orange, alpha=0.5, label="Normal Distribution", guidefontsize=18, tickfontsize=16, legendfontsize=16)
histogram!(p2, rand(Cauchy(0, 1), 1000), color=:green, alpha=0.5, label="Cauchy Distribution")
xlabel!("Value")
ylabel!("Count")
plot!(p2, size=(600, 450))
xlims!(-20, 20)

display(p1)
display(p2)
```


## Quantile-Quantile (Q-Q) Plots

:::: {.columns}
::: {.column width=50%}
Particularly with small sample sizes, just staring at data can be unhelpful.

Q-Q plots are useful for checking goodness of fit of a proposed distribution.
:::

::: {.column width=50%}
```{julia}
#| label: fig-norm-qq
#| code-fold: true
#| code-overflow: wrap
#| echo: true

samps = rand(Normal(0, 3), 20)
qqplot(Normal, samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)
xlabel!("Theoretical Quantiles")
ylabel!("Empirical Quantiles")
plot!(size=(500, 450))
```
:::
::::

## Q-Q Plot Example

```{julia}
#| label: fig-cauchy-qq
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Q-Q Plot for Cauchy Data and Normal Distribution"
#| fig-subcap: 
#|  - "Normal vs Cauchy Distribution"
#|  - "Q-Q Plot"

## generate fat-tailed residuals
cauchy_samps = rand(Cauchy(0, 1), 20)

# make plots
# scatterplot of observations
p1 = plot(Normal(0, 2), linewidth=3, color=:green, label="Normal Distribution", yaxis=false, tickfontsize=16, guidefontsize=18, legendfontsize=18)
plot!(p1, Cauchy(0, 1), linewidth=3, color=:orange, linestyle=:dash, label="Cauchy Distribution")
xlims!(p1, (-10, 10))
xlabel!("Value")
plot!(p1, size=(500, 450))

# densities of residual distributions
p2 = qqplot(Normal, cauchy_samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)
xlabel!(p2, "Theoretical Quantiles")
ylabel!(p2, "Empirical Quantiles")
plot!(p2, size=(500, 450))

display(p1)
display(p2)
```

## Predictive Checks

How well do projections match data [@Gelman2004-gh]?

Can be quantitative or qualitative.


## Predictive Check: EBM Example

```{julia}
#| echo: false
#| output: false

# p are the model parameters, σ the standard deviation of the normal errors, y is the data, m the model function
function log_likelihood(p, σ, y, m)
    y_pred = m(p)
    ll = sum(logpdf.(Normal.(y_pred, σ), y))
end

ebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)

# maximize log-likelihood within some range
# important to make everything a Float instead of an Int 
lower = [1.0, 50.0, 0.0, 0.0]
upper = [4.0, 150.0, 2.0, 10.0]
p0 = [2.0, 100.0, 1.0, 1.0]
result = Optim.optimize(params -> -log_likelihood(params[1:end-1], params[end], temp_obs, ebm_wrap), lower, upper, p0)
θ = result.minimizer
```

```{julia}
#| output: true
#| echo: true
#| label: fig-temp-realizations
#| fig-align: center
#| fig-cap: Comparison of best fit with uncertain realization for the EBM with normal residuals.
#| code-fold: true

# set number of sampled simulations
n_samples = 1000
residuals = rand(Normal(0, θ[end]), (n_samples, length(temp_obs)))
model_out = ebm_wrap(θ[1:end-1])
# this uses broadcasting to "sweep" the model simulation across the sampled residual matrix
model_sim = residuals .+ model_out' # need to transpose the model output vector due to how Julia treats vector dimensions

q_90 = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim,; dims=1) # compute 90% prediction interval

plot(hind_years, model_out, color=:red, linewidth=3, label="Model Simulation", ribbon=(model_out .- q_90[1, :], q_90[2, :] .- model_out), fillalpha=0.3, xlabel="Year", ylabel="Temperature anomaly (°C)", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)
scatter!(hind_years, temp_obs, color=:black, label="Data")
ylims!(-0.5, 1.2)
```

## Predictive Checks

Can also do predictive checks with summary statistics:

- Return periods
- Maximum/minimum values
- Predictions for out-of-sample data

# Correlation and Auto-Correlation

## What Is Correlation?

**Correlation** refers to whether two variables increase or decrease simultaneously.

Typically measured with Pearson's coefficient:

$$r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \in (-1, 1)$$

## Correlation Examples

```{julia}
#| label: fig-correlation
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 3
#| fig-cap: "Independent vs. Correlated Normal Variables"
#| fig-subcap: 
#|  - "Independent Variables"
#|  - "Correlated Variables ($r=0.7$)"
#|  - "Anti-Correlated Variables ($r=-0.7$)"

# sample 1000 independent variables from a given normal distribution
sample_independent = rand(Normal(0, 1), (2, 1000))
p1 = scatter(sample_independent[1, :], sample_independent[2, :], label=:false, title="Independent Variables", tickfontsize=14, titlefontsize=18, guidefontsize=18)
xlabel!(p1, L"$x_1$")
ylabel!(p1, L"$x_2$")
plot!(p1, size=(400, 500))

# sample 1000 correlated variables, with r=0.7
sample_correlated = rand(MvNormal([0; 0], [1 0.7; 0.7 1]), 1000)
p2 = scatter(sample_correlated[1, :], sample_correlated[2, :], label=:false, title=L"Correlated ($r=0.7$)", tickfontsize=14, titlefontsize=18, guidefontsize=18)
xlabel!(p2, L"$x_1$")
ylabel!(p2, L"$x_2$")
plot!(p2, size=(400, 500))

# sample 1000 anti-correlated variables, with r=-0.7
sample_anticorrelated = rand(MvNormal([0; 0], [1 -0.7; -0.7 1]), 1000)
p3 = scatter(sample_anticorrelated[1, :], sample_anticorrelated[2, :], label=:false, title=L"Anticorrelated ($r=-0.7$)", tickfontsize=14, titlefontsize=18, guidefontsize=18)
xlabel!(p3, L"$x_1$")
ylabel!(p3, L"$x_2$")
plot!(p3, size=(400, 500))

display(p1)
display(p2)
display(p3)
```

## Correlation vs. Causation

![XKCD 552](https://imgs.xkcd.com/comics/correlation_2x.png){fig-align="center"}

::: {.caption}
Source: [XKCD #552](https://xkcd.com/552/)
:::

## "Correlation Does Not Imply Causation"

- Data can be correlated randomly (a spurious correlation)
- Data can be correlated due to a mutual causal factor


## Spurious Correlations

![Spurious Correlation Example](https://www.tylervigen.com/spurious/correlation/image/7602_the-distance-between-uranus-and-mercury_correlates-with_biomass-power-generated-in-united-states.svg){fig-align="center"}

::: {.caption}
Source: [Spurious Correlations #7602](https://www.tylervigen.com/spurious/correlation/7602_the-distance-between-uranus-and-mercury_correlates-with_biomass-power-generated-in-united-states)
:::

## Correlated Data Likelihood

- Marginal distributions normal: use a Multivariate Normal distribution
- Otherwise: Use [copulas](https://en.wikipedia.org/wiki/Copula_(probability_theory)) to "glue" marginal distributions together.

## Autocorrelation

An important concept for time series is **autocorrelation** between $y_t$ and $y_{t-1}$.

```{julia}
#| label: fig-autocorrelation
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Autocorrelated vs. Independent Samples"
#| fig-subcap: 
#|  - "Independent Variables"
#|  - "Autocorrelated Variables"

# sample independent series from a given normal distribution
sample_independent = rand(Normal(0, 1), 50)
p1 = plot(sample_independent, linewidth=3, ylabel=L"$y$", xlabel=L"$t$", title="Independent Series", guidefontsize=18, legend=:false, tickfontsize=16, titlefontsize=18)
plot!(p1, size=(500, 300))

# sample an autocorrelated series
sample_ar = zeros(50)
sample_ar[1] = rand(Normal(0, 1 / sqrt(1-0.6^2)))
for i = 2:50
    sample_ar[i] = 0.6 * sample_ar[i-1] + rand(Normal(0, 1))
end
p2 = plot(sample_ar, linewidth=3, ylabel=L"$y$", xlabel=L"$t$", title="Autocorrelated Series", guidefontsize=18, legend=:false, tickfontsize=16, titlefontsize=18)
plot!(p2, size=(500, 300))

display(p1)
display(p2)
```

## Lagged Regression

```{julia}
#| label: fig-autocorrelation-2
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Autocorrelated vs. Independent Samples"
#| fig-subcap: 
#|  - "Independent Variables"
#|  - "Autocorrelated Variables"

# independent samples
dat = DataFrame(Y=sample_independent[2:end], X=sample_independent[1:end-1])
fit = lm(@formula(Y~X), dat)
pred = predict(fit,dat)
p1 = scatter(sample_independent[1:end-1], sample_independent[2:end], label=:false, title="Independent Variables: r=$(round(coef(fit)[2], digits=2))", tickfontsize=16, titlefontsize=18, guidefontsize=18)
plot!(p1, dat.X, pred, linewidth=2, label=:false)
ylabel!(p1, L"$y_t$")
xlabel!(p1, L"$y_{t-1}$")
plot!(p1, size=(600, 500))

# autocorrelated
dat = DataFrame(Y=sample_ar[2:end], X=sample_ar[1:end-1])
fit = lm(@formula(Y~X), dat)
pred = predict(fit,dat)
p2 = scatter(sample_ar[1:end-1], sample_ar[2:end], label=:false, title="Autocorrelated Variables: r=$(round(coef(fit)[2], digits=2))", tickfontsize=16, titlefontsize=18, guidefontsize=18)
plot!(p2, dat.X, pred, linewidth=2, label=:false)
ylabel!(p2, L"$y_t$")
xlabel!(p2, L"$y_{t-1}$")
plot!(p2, size=(600, 500))

display(p1)
display(p2)
```

## Autocorrelation Function

Many modern programming languages have **autocorrelation functions** (e.g. `StatsBase.autocor()` in Julia) which calculates the autocorrelation at varying lags.

```{julia}
#| output: true
#| echo: true
#| output-location: column

autocor(sample_ar, 1:10)
```

## Autocorrelation Function

But the autocorrelation function just computes the autocorrelations at every lag. Why is this a modeling problem?

## Partial Autocorrelation Function

This is solved by using the **partial autocorrelation function** (*e.g.* `StatsBase.pacf()`):

```{julia}
#| output: true
#| echo: true
#| output-location: column

pacf(sample_ar, 1:10)
```

## Autoregressive Models

An autoregressive-with-lag-$p$ (**AR($p$)**) model:
$$y_t = \sum_{i=1}^{p} \rho_i y_{t-i} + \varepsilon_t$$

::: {.fragment .fade-in}
***Example***: An AR(1) model: 
$$
\begin{gather*}
y_t = \rho y_{t-1} + \varepsilon_t \\
\varepsilon_t \sim \mathcal{N}(0, \sigma)
\end{gather*}
$$  
:::

## AR(1) Likelihood

There are two ways to write down an AR(1) likelihood.

1. "Whiten" the series:
   
$$
\begin{gather*}
\varepsilon_t = y_t - y_{t-1} \sim \mathcal{N}(0, \sigma)\\
y_1 \sim \mathcal{N}\left(0, \frac{\sigma}{\sqrt{1-\rho^2}}\right)
\end{gather*}
$$

## AR(1) Likelihood

2. Joint likelihood:

$$
\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \Sigma) \qquad
\Sigma = \begin{pmatrix}\frac{\sigma^2}{1-\rho^2} & \rho & \rho^2 & \ldots & \rho^{T-1} \\ \rho & \frac{\sigma^2}{1-\rho^2} & \rho & \ldots & \rho^{T-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \ldots & \frac{\sigma^2}{1-\rho^2} \end{pmatrix}
$$

# Model-Data Discrepancy

## Model-Data Discrepancy

**Model-data discrepancy**: systematic disagreements between model and the modeled system state [@Brynjarsdottir2014-ve].

Let $F(\mathbf{x}; \mathbf{\theta})$ be the simulation model:

- $\mathbf{x}$ are the "control variables";
- $\mathbf{\theta}$ are the "calibration variables".

## Model-Data Discrepancy

If $\mathbf{y}$ are the "observations,"" we can model these as:
$$\mathbf{y} = z(\mathbf{x}) + \mathbf{\varepsilon},$$
where

- $z(\mathbf{x})$ is the "true" system state at control variable $\mathbf{x}$;
- $\mathbf{\varepsilon}$ are observation errors.

## Model-Data Discrepancy

Then the *discrepancy* $\zeta$ between the simulation and the modeled system is:
$$\zeta(\mathbf{x}; \mathbf{\theta}) = z(\mathbf{x}) - F(\mathbf{x}; \mathbf{\theta}).$$

::: {.fragment .fade-in}
Then observations can be written as:

$$\mathbf{y} =  F(\mathbf{x}; \mathbf{\theta}) + \zeta(\mathbf{x}; \mathbf{\theta}) + \mathbf{\varepsilon}.$$
:::

## Simple Discrepancy Example

Common to model observation errors as normally-distributed: $\varepsilon_t \sim \mathcal{N}(0, \omega)$

If the discrepancy is also i.i.d. normal: $\zeta_t \sim \mathcal{N}(0, \sigma)$.

Residuals: $$\zeta_t + \varepsilon_t \sim \mathcal{N}(0, \sqrt{\omega^2 + \sigma^2})$$

## Complex Discrepancy Example

Now suppose the discrepancy is AR(1).

$$\begin{gather*}
\mathbf{\zeta} + \mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \Sigma) \\
\Sigma = \begin{pmatrix}\frac{\sigma^2}{1-\rho^2} + {\color{red}\omega^2} & \rho & \rho^2 & \ldots & \rho^{T-1} \\ \rho & \frac{\sigma^2}{1-\rho^2} + {\color{red}\omega^2} & \rho & \ldots & \rho^{T-2} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \ldots & \frac{\sigma^2}{1-\rho^2} + {\color{red}\omega^2} \end{pmatrix}
\end{gather*}
$$

## Fitting Discrepancies

In many cases, separating discrepancy from observation error is tricky without prior information about variances.

**Example**: $$\zeta_t + \varepsilon_t \sim \mathcal{N}(0, \sqrt{\omega^2 + \sigma^2})$$

Not being able to separate $\omega$ from $\sigma$ is a problem called ***non-identifiability***.

## Discrepancies and Machine Learning

Can use machine learning models to "emulate" complex discrepancies and error structures.

More on ML as an emulation/error tool later in the semester.


# Key Points and Upcoming Schedule

## Key Points: EDA

- EDA helps identify a good or bad probability model;
- **No black-box workflow**: make plots, compare samples from proposed model to data, try to be skeptical

## Key Points: Correlation

- Check for correlations, but think mechanistically.
- Use partial autocorrelation functions to find autoregressive model orders.

## Key Points: Discrepancy

- Separate out "model bias" from observation errors.
- Often neglected: can be hard to fit without prior information on parameters.
- Use provided observation error variances when available to avoid non-identifiability.

## Next Class(es)

**Wednesday**: Bayesian Statistics and Prior Information

## Assessments

**Friday**: Exercise 1 due by 9pm.

HW2 assigned this week (will announce on Ed), due 2/23 by 9pm.

# References

## References

