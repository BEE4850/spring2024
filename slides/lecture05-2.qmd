---
title: "Data Visualization"
subtitle: "Lecture 09"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 21, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
engine: julia
julia:
    exeflags: ["+1.10.4"]          
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using Plots
using LaTeXStrings
using ColorSchemes
using Turing
using RDatasets
using GLM
using DataFrames
using Measures

Random.seed!(1)
```

# Last Class(es)

## Two Ways To Frame "Extreme" Values

1. "Block" extremes, *e.g.* annual maxima (**block maxima**)?
2. Values which exceed a certain threshold (**peaks over threshold**)?

## Two Ways To Frame "Extreme" Values

- Block Maxima: Generalized Extreme Value (GEV) distributions.
- Peaks-Over-Thresholds: Generalized Pareto distributions (GP) (plus maybe Poisson processes).
- Statistical models are highly sensitive to details: shape parameters $\xi$, thresholds $u$, etc.
- **Models assume independent variables.** 

# Data Visualization: Basic Principles

## Purposes of Visualizing Data

::: {.incremental}
- Exploratory Analysis
- Communication
- Interpretation
:::

## Quantitative Summaries Can Be Insufficient

```{julia}
#| fig-align: center
#| label: fig-anscombe
#| fig-cap: Anscombe's Quartet
#| code-fold: true
#| echo: true

# load Anscombe's Quartet data
df = dataset("datasets", "anscombe")

model1 = lm(@formula(Y1 ~ X1), df)
model2 = lm(@formula(Y2 ~ X2), df)
model3 = lm(@formula(Y3 ~ X3), df)
model4 = lm(@formula(Y4 ~ X4), df)

yHat(model, X) = coef(model)' * [ 1 , X ]
xlims = [0, 20]

p1 = scatter(df.X1, df.Y1, c=:blue, msw=0, ms=8)
p1 = plot!(xlims, [yHat(model1, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)

p2 = scatter(df.X2, df.Y2, c=:blue, msw=0, ms=8)
p2 = plot!(xlims, [yHat(model2, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)

p3 = scatter(df.X3, df.Y3, c=:blue, msw=0, ms=8)
p3 = plot!(xlims, [yHat(model3, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)

p4 = scatter(df.X4, df.Y4, c=:blue, msw=0, ms=8)
p4 = plot!(xlims, [yHat(model4, x) for x in xlims], c=:red, msw=0, xlims=(xlims), linewidth=2)

plot(p1, p2, p3, p4, layout = (1,4), xlims=(0,20), ylims=(0,14), 
	legend=:none, xlabel = "x", ylabel = "y",
    tickfontsize=16, guidefontsize=18,
    left_margin=5mm, bottom_margin=10mm, right_margin=5mm)
plot!(size=(1200, 400))
```

## Challenges for Effective Visualization

::: {.incremental}
- Limits From Cognitive Processes
- No "Optimal" Visualization
- Temptation To Overload Figures
- Easy to "Lie" About The Data
:::

## Further Challenges

Following @Munzner2014-pj:

::: {.incremental}
- Possible designs are a bad match with human perceptual and cognitive systems;
- Possible designs are a bad match with the intended task;
- Only a small number of possibilities are reasonable choices;
- "**Randomly choosing possibilities is a bad idea because the odds of finding a very good solution are very low**."
:::

## What Can Go Wrong?

@Healy2018-zx:

::: {.incremental}
- Bad Taste;
- Bad Data;
- Bad Perception
:::

## Remember: Data Never Speaks For Itself

Data must be understood in a particular context. You need to understand your data and what it says (or does not say!) based on your hypotheses.

::: {.incremental}
- What question(s) does your data address?
- What transformations make the representation of the data as salient as possible?
- What scales or channels are most appropriate?
:::

## Some Caveats

::: {.incremental}
- There is no recipe to effective visualization. Everything depends on your data and the story you want to tell.
- This also means that defaults from data visualization packages are usually bad.
- These principles are largely based on Western (American/European) norms. 
- A lot of these guidelines are based on average outcomes, there is likely to be a lot of individual variation.
:::

# Human Perception and Cognition

## Stages of Human Visual Perception

::: {.incremental}
1. Rapid, pre-attentive *parallel* processing to extract basic features;
2. Slow serial processing for extraction of patterns;
3. Goal-based retention of a few pieces of information in *working memory* related to a question at hand.
:::

## Working Memory Is Limited!

Estimates of the number of "bits" we can keep in working memory vary, but:

::: {.incremental}
- Limit is small;
- Exceeding limit results in cognitive load;
- Working memory is subject to "change blindness"
:::

::: {.fragment .fade-in}
**The more cognitive work you ask of your viewer, the less they are likely to take away and retain!**
:::

## Gestalt Principles

The Gestalt school of psychology identified several principles of perception.

Core idea: **Humans are very good at finding structure.**

::: {.fragment .fade-in}
As a result, you need to evaluate the totality of a visual field, not just each component.
:::

## Gestalt Principles

:::: {.columns}

::: {.column width=40%}
- Proximity
- Similarity
- Parallelism
- Common Fate
- Common Region
- Continuity
- Closure
:::

::: {.column width=60%}
![Illustration of Gestalt principles](figures/gestalt-01.png)

::: {.caption}
Illustration of several Gestalt principles. Adapted from @Healy2018-zx.
:::

:::
::::

## Don't Add Unnecessary Artifacts!

:::: {.columns}
::: {.column width=30%}
Unnecessary artifacts can be "chartjunk."

But worse, they might mislead the viewer.
:::
::: {.column width=70%}
```{julia}
#| echo: false
#| output: false

# create trend for data
x = rand(Uniform(0, 20), 50)
y = 5 .+ 2 * x
# sample and add noise
ε = rand(Normal(0, 5), 50)
y .+= ε

@model function linear_regression(x, y)
    # set priors
    σ ~ truncated(Normal(0, 1); lower=0)      # <1>
    a ~ Normal(0, 1)                           # <2>
    b ~ Normal(0, 1)                           # <2>

    # compute the likelihood
    for i = 1:length(y)                         # <3>
        # compute the mean value for the data point
        μ = a + b * x[i]
        y[i] ~ Normal(μ, σ)
    end
end

# set up the sampler
model = linear_regression(x, y)     # <1>
n_chains = 4                         # <2>
n_per_chain = 5000                  # <3>
chain = sample(model, NUTS(), MCMCThreads(), n_per_chain, n_chains, drop_warmup=true)

# plot fit
function mc_predict_regression(x, chain)
    # get the posterior samples
    a = Array(group(chain, :a))     # <1>
    b = Array(group(chain, :b))     # <1>
    σ = Array(group(chain, :σ))   # <1>

    # loop and generate alternative realizations
    μ = a' .+ x * b'
    y = zeros((length(x), length(a)))
    for i = 1:length(a)
        y[:, i] = rand.(Normal.(μ[:, i], σ[i]))
    end
    return y
end

x_pred = 0:20
y_pred = mc_predict_regression(x_pred, chain)

# get the boundaries for the 95% prediction interval and the median
y_ci_low = quantile.(eachrow(y_pred), 0.025)
y_ci_hi = quantile.(eachrow(y_pred), 0.975)
y_med = quantile.(eachrow(y_pred), 0.5)
```

```{julia}
#| echo: true
#| code-fold: true

p1 = plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L"$x$", ylabel=L"$y$", fillalpha=0.3, fillcolor=:blue, label="95% Prediction Interval", legend=:topleft, linealpha=0, legendfontsize=12, tickfontsize=14, guidefontsize=14) 
plot!(p1, x_pred, y_med, color=:blue, label="Prediction Median")
scatter!(p1, x, y, color=:red, markershape=:x, label="Data")

p2 = plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L"$x$", ylabel=L"$y$", fillalpha=0.3, fillcolor=:blue, label="95% Prediction Interval", legend=:topleft, linealpha=0, legendfontsize=12, tickfontsize=14, guidefontsize=14) 
plot!(p2, x_pred, y_med, color=:blue, label="Prediction Median")
plot!(p2, x, y, color=:red, seriestype=:line, label=:false)
scatter!(p2, x, y, color=:red, markershape=:x, label="Data")

plot(p1, p2, layout=(1, 2), size=(800, 600))
```
:::
::::

# Channels For Encoding Information

## Channels

A *channel* is a mechanism for encoding information.

::: {.fragment .fade-in}
Examples:

- Color (Hue/Saturation/Luminescence)
- Position (1D/2D/3D)
- Size (Length/Area/Volume)
- Angle
:::

## Ordered vs. Categorical Attributes

The channels available depend on the type of attribute:

- **Ordered** attributes can be 
  - *Ordinal*: Ranking, no meaning to distance;
  - *Quantitative*: Measure of magnitude which supports arithmetic comparison;
- **Categorical** attributes are unordered.

## Channel Effectiveness: Ordered Data

:::: {.columns}
::: {.column width=75%}
![](figures/ordered-channels.png){fig-align=center}
:::
::: {.column width=25%}
::: {.caption}
Channels for ordered data, arranged top-to-bottom from more to less effective (channels in the right column are less effective than those in the left). Modified from @Healy2018-zx after @Munzner2014-pj.
:::
:::
::::

## Channel Effectiveness: Categorical Data

:::: {.columns}
::: {.column width=75%}
![](figures/categorical-channels.png){fig-align=center width=5in}
:::
::: {.column width=25%}
::: {.caption}
Channels for categorical data, arranged top-to-bottom from more to less effective. Modified from Healy (2018) after Munzer (2014).
:::
:::
::::

## Preattentive Popout

:::: {.columns}
::: {.column width=30%}
Try to make your key features "pop out" to the viewer during the pre-attentive scan.

::: {.caption}
Searching for the blue circle becomes harder. Adapted from @Healy2018-zx.
:::
:::
::: {.column width=70%}

```{julia}
#| code-fold: true
#| echo: true
npt = 20
dist = Distributions.Product(Uniform.([0, 0], [1, 1]))
pts = Tuple.(eachcol(rand(dist, npt)))
blueidx = rand(1:npt)
p1 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title="Color Only, N=20", framestyle=:box)
scatter!(p1, pts[blueidx, :], color=:blue, markersize=5)

npt = 100
pts = Tuple.(eachcol(rand(dist, npt)))
blueidx = rand(1:npt)
p2 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title="Color Only, N=100", framestyle=:box)
scatter!(p2, pts[blueidx, :], color=:blue, markersize=5)

npt = 20
pts = Tuple.(eachcol(rand(dist, npt)))
blueidx = rand(1:npt)
p3 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title="Shape Only, N=20", framestyle=:box)
scatter!(p3, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)

npt = 100
pts = Tuple.(eachcol(rand(dist, npt)))
blueidx = rand(1:npt)
p4 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title="Shape Only, N=100", framestyle=:box)
scatter!(p4, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)

plot(p1, p2, p3, p4, layout=(2, 2), size=(800, 500))
```

:::
::::


## Channel Interference

:::: {.columns}
::: {.column width=60%}
When using multiple channels, be careful about *interference*: reducing the effectiveness of both channels. 
:::
::: {.column width=40%}
```{julia}
npt = 100
dist = Distributions.Product(Uniform.([0, 0], [1, 1]))
bluetri = Tuple.(eachcol(rand(dist, Int(npt / 2) - 1)))
bluecirc = Tuple(rand(dist, 1))
redcirc = Tuple.(eachcol(rand(dist, Int(npt / 4))))
redtri = Tuple.(eachcol(rand(dist, Int(npt / 4))))
scatter(redcirc, color=:red, markershape=:circle, xticks=:false, yticks=:false, legend=:false, markersize=5, title="Color and Shape, N=100", framestyle=:box)
scatter!(redtri, color=:red, markershape=:utriangle, markersize=5)
scatter!(bluetri, color=:blue, markershape=:utriangle, markersize=5)
scatter!(bluecirc, color=:blue, markershape=:circle, markersize=5)
plot!(size=(400, 400))
```
:::
::::

## Color Schemes

Different color schemes are appropriate depending on whether the data is *sequential*, *divergent*, or *unordered*.

::: {.callout-important}
### Appropriate Color Schemes

Color schemes should be *perceptually uniform* to preserve a mapping between changes in perceived colors and changes in attribute values. 

Try to also choose color schemes which avoid confusing people who are color blind.
:::


## Color Schemes

**Good news**: Most plotting libraries include a wide variety of perceptually uniform, color-blind safe color schemes.

**Bad news**: These are not usually the defaults (in particular, avoid "rainbow" color schemes).

## Sequential Color Schemes

Sequential schemes change in intensity from low to high as the value changes. 

```{julia}
#| fig-align: center
ColorSchemes.Blues_7
```

## Divergent Color Schemes

Divergent schemes intensify in two directions from a zero or mean value.

```{julia}
reverse(ColorSchemes.PRGn_7)
```

## Unordered Color Schemes

Unordered schemes are appropriate for categorical data. 

```{julia}
reverse(ColorSchemes.Dark2_7)
```

# Some Examples

## Thoughts On This Plot?

::: {.center}
![First Street Foundation Return Period Trends](figures/shu-etal-returnperiods.png)
:::

::: {.caption}
Source: @Shu2023-ht
:::

## How About This One?

::: {.center}
![Trump Polling Average vs. Employment in Swing States](figures/swingstate-morgan.jpeg)
:::

::: {.caption}
Source: [Joe Weisenthal](https://twitter.com/TheStalwart/status/1309582619290595328)
:::

## Last One!

::: {.center}
![Modeled Flood Risk vs. Perception](figures/bakkensen-floodrisk.jpeg){width=55%}
:::

::: {.caption}
Source: @Bakkensen2022-xj
:::


# Key Points, Upcoming Schedule, and References

## Recommendations

::: {.incremental}
1. Don't add extraneous artifacts.
2. Make key features "pop out," or annotate them.
3. Summarize data to reduce complexity.
4. Try to prioritize high-effectiveness channels.
5. Don't use 3d!
6. Mix channels sparingly (but redundancy is good!).
7. **Is the figure an improvement over a table?**
:::

## But the Biggest Recommendation of All...

**Be intentional with your choices based on your storytelling goal!**

Relying on defaults will usually steer you wrong, and all "rules" can be broken if they help you tell your story more effectively.

## What About Exploratory Analysis?

When exploring data, try lots of things. 

- Don't over-interpret one visualization. 
- Try to rely on hypotheses about what you might see instead of dredging through the data.

## Upcoming Schedule

**Monday**: *February Break!*

**Next Wednesday**: In-Class Figure Discussion

## Assessments

**Friday**: 

- Submit figures for discussion (Exercise 5)
- HW2 Due


## References