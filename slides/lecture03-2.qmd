---
title: "Model-Data Discrepancy and Bayesian Statistics"
subtitle: "Lecture 06"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 07, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
engine: julia
julia:
    exeflags: ["+1.10.4"]          
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using CSV
using DataFrames
using DataFramesMeta
using Dates
using Optim

Random.seed!(1)
```

```{julia}
#| output: false
#| echo: false

# use default values of S=3.2°C, d=100m, α=1.3
function ebm(rf_nonaerosol, rf_aerosol; p=(3.2, 100, 1.3))
    # set up model parameters
    S, d, α = p # this unpacks the parameter tuple into variables
    F2xCO₂ = 4.0 # radiative forcing [W/m²] for a doubling of CO₂
    λ = F2xCO₂ / S

    c = 4.184e6 # heat capacity/area [J/K/m²]
    C = c*d # heat capacity of mixed layer (per area)
    F = rf_nonaerosol + α*rf_aerosol # radiative forcing
    Δt = 31558152. # annual timestep [s]

    T = zero(F)
    for i in 1:length(F)-1
        T[i+1] = T[i] + (F[i] - λ*T[i])/C * Δt
    end
    
    # return after normalizing to reference period
    return T .- mean(T[1:20])
end

# Dataset from https://zenodo.org/record/3973015
# The CSV is read into a DataFrame object, and we specify that it is comma delimited
forcings_all_85 = CSV.read("data/climate/ERF_ssp585_1750-2500.csv", DataFrame, delim=",")

# Separate out the individual components
forcing_co2_85 = forcings_all_85[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_85 = forcings_all_85[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_85 = forcings_all_85[!,"aerosol-cloud_interactions"]
forcing_aerosol_85 = forcing_aerosol_rad_85 + forcing_aerosol_cloud_85
forcing_total_85 = forcings_all_85[!,"total"]
forcing_non_aerosol_85 = forcing_total_85 - forcing_aerosol_85
forcing_other_85 = forcing_total_85 - (forcing_co2_85 + forcing_aerosol_85)

forcings_all_26 = CSV.read("data/climate/ERF_ssp126_1750-2500.csv", DataFrame, delim=",")

# Separate out the individual components
forcing_co2_26 = forcings_all_26[!,"co2"]
# Get total aerosol forcings
forcing_aerosol_rad_26 = forcings_all_26[!,"aerosol-radiation_interactions"]
forcing_aerosol_cloud_26 = forcings_all_26[!,"aerosol-cloud_interactions"]
forcing_aerosol_26 = forcing_aerosol_rad_26 + forcing_aerosol_cloud_26
forcing_total_26 = forcings_all_26[!,"total"]
forcing_non_aerosol_26 = forcing_total_26 - forcing_aerosol_26
forcing_other_26 = forcing_total_26 - (forcing_co2_26 + forcing_aerosol_26)

t = time_forcing = Int64.(forcings_all_85[!,"year"]) # Ensure that years are interpreted as integers
sim_years = 1850:2100 # model years for projections
sim_idx = indexin(sim_years, t)

temps = CSV.read("data/climate/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv", DataFrame, delim=",")

time_obs = temps[:, 1]
temp_obs = temps[:, 2]
temp_lo = temps[:, 3]
temp_hi = temps[:, 4]

# generate simulations
hind_years = 1850:2020 # model years to simulate for fitting
sim_years = 1850:2100 # model years for projections
idx = indexin(hind_years, t) # find indices in t vector of simulation years
# since we specified default values for p, those are used for the parameters
temp_default = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx]) 

temp_obs = temp_obs[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_lo = temp_lo[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_hi = temp_hi[indexin(hind_years, time_obs)] # filter to simulated years for plotting
temp_obs = temp_obs .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_lo = temp_lo .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_hi = temp_hi .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model
temp_sd = (temp_hi - temp_lo) / 2 # estimate standard deviation using 95% CI
```

# Last Class(es)

## Model-Data Discrepancy

Systematic mismatch between model and system state (not observation error!)

$$\mathbf{y} =  \underbrace{F(\mathbf{x}; \mathbf{\theta})}_\text{simulation} + \underbrace{\zeta(\mathbf{x}; \mathbf{\theta})}_\text{discrepancy} + \underbrace{\mathbf{\varepsilon}}_\text{observation error}.$$


## Non-Identifiability

Inability to distinguish between statistical parameters, *e.g.*

$$\zeta_t + \varepsilon_t \sim \mathcal{N}(0, \sqrt{\omega^2 + \sigma^2})$$



# Discrepancy Example

## EBM With AR(1) Discrepancy

$$
\begin{gather*}
y_t = \text{EBM}(F_t; \theta) + \zeta_t + \varepsilon_t \\
\zeta_t = \rho \zeta_{t-1} \sim \mathcal{N}(0, \sigma)\\
\varepsilon_t \sim \mathcal{N}(0, \omega_t)
\end{gather*}
$$

where $\omega_t$ is estimated as half the 95% CI of the temperature data.

## Covariance Matrix

The likelihood is $\zeta + \varepsilon \sim N(\mathbf{0}, \Sigma)$.

$$
\begin{gather*}
\Sigma = V + D \\
D = \text{diag}(\omega_t^2), \quad V = \frac{\sigma^2}{1-\rho^2}\begin{pmatrix}1 & \rho & \ldots & \rho^{T-1} \\ \rho & 1 & \ldots & \rho^{T-2} \\ \rho^2 & \rho & \ldots & \rho^{T-3} \\ \vdots & \vdots & \ddots & \vdots \\ \rho^{T-1} & \rho^{T-2} & \ldots & 1\end{pmatrix}
\end{gather*}
$$

## In Code...

```{julia}
#| echo: true
#| output: true
#| code-line-numbers: "|3|4,5|6,7,8,9"

# p are the model parameters, σ the standard deviation of the AR(1) errors, ρ is the autocorrelation coefficient, y is the data, m the model function
function ar_covariance_mat(σ, ρ, y_err)
    H = abs.((1:length(y_err)) .- (1:(length(y_err)))') # compute the outer product to get the correlation lags
    ζ_var = σ^2 / (1-ρ^2)
    Σ = ρ.^H * ζ_var
    for i in 1:length(y_err)
        Σ[i, i] += y_err[i]^2
    end
    return Σ
end
```

## Maximize Likelihood

```{julia}
#| echo: true
#| output: false
function ar_discrep_log_likelihood(p, σ, ρ, y, m, y_err)
    y_pred = m(p)
    residuals = y_pred .- y
    Σ = ar_covariance_mat(σ, ρ, y_err)
    ll = logpdf(MvNormal(zeros(length(y)), Σ), residuals)
    return ll
end

ebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)

# maximize log-likelihood within some range
# important to make everything a Float instead of an Int 
lower = [1.0, 50.0, 0.0, 0.0, -1.0]
upper = [4.0, 150.0, 2.0, 10.0, 1.0]
p0 = [2.0, 100.0, 1.0, 1.0, 0.0]
result = Optim.optimize(params -> -ar_discrep_log_likelihood(params[1:end-2], params[end-1], params[end], temp_obs, ebm_wrap, temp_sd), lower, upper, p0)
θ_discrep = result.minimizer
```

```{julia}
#| echo: false
#| output: false

function ar_log_likelihood(p, σ, ρ, y, m)
    y_pred = m(p)
    ll = 0 # initialize log-likelihood counter
    residuals = y_pred .- y
    ll += logpdf(Normal(0, σ/sqrt(1-ρ^2)), residuals[1])
    for i = 1:length(residuals)-1
        residuals_whitened = residuals[i+1] - ρ * residuals[i]
        ll += logpdf(Normal(0, σ), residuals_whitened)
    end
    return ll
end

# maximize log-likelihood within some range
# important to make everything a Float instead of an Int 
lower = [1.0, 50.0, 0.0, 0.0, -1.0]
upper = [4.0, 150.0, 2.0, 10.0, 1.0]
p0 = [2.0, 100.0, 1.0, 1.0, 0.0]
result = Optim.optimize(params -> -ar_log_likelihood(params[1:end-2], params[end-1], params[end], temp_obs, ebm_wrap), lower, upper, p0)
θ_ar1 = result.minimizer
```

```{julia}
#| echo: false
#| output: false

# p are the model parameters, σ the standard deviation of the normal errors, y is the data, m the model function
function log_likelihood(p, σ, y, m)
    y_pred = m(p)
    ll = sum(logpdf.(Normal.(y_pred, σ), y))
end

ebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)

# maximize log-likelihood within some range
# important to make everything a Float instead of an Int 
lower = [1.0, 50.0, 0.0, 0.0]
upper = [4.0, 150.0, 2.0, 10.0]
p0 = [2.0, 100.0, 1.0, 1.0]
result = Optim.optimize(params -> -log_likelihood(params[1:end-1], params[end], temp_obs, ebm_wrap), lower, upper, p0)
θ = result.minimizer
```

## Comparison of Model Fits

:::: {.columns}
::: {.column width=33%}
***Normal IID***:
```{julia}
θ
```
:::
::: {.column width=33%}
***Total-Residual AR(1)***:
```{julia}
θ_ar1
```
:::
::: {.column width=33%}
***Discrepancy AR(1)***:
```{julia}
θ_discrep
```
:::
::::

## Hindcast

```{julia}
#| fig-align: center
#| label: fig-hindcast
#| fig-cap: Hindcasts from different EBM fits.

n_samples = 10000

# discrepancy samples
Σ = ar_covariance_mat(θ_discrep[4], θ_discrep[5], temp_sd)
residuals_discrep = rand(MvNormal(zeros(length(hind_years)), Σ), n_samples)
model_discrep = ebm_wrap(θ_discrep[1:end-2])
model_sim_discrep = (residuals_discrep .+ model_discrep)'
q90_discrep = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_discrep; dims=1) # compute 90% prediction interval

# iid samples
residuals = rand(Normal(0, θ[end]), (n_samples, length(temp_obs)))
model_iid = ebm_wrap(θ[1:end-1])
# this uses broadcasting to "sweep" the model simulation across the sampled residual matrix
model_sim_iid = residuals .+ model_iid' # need to transpose the model output vector due to how Julia treats vector dimensions
q90_iid = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval

# ar1 simulations
residuals_ar1 = zeros(n_samples, length(hind_years))
residuals_ar1[:, 1] = rand(Normal(0, θ_ar1[end-1] / sqrt(1-θ_ar1[end]^2)), n_samples)
for i = 2:size(residuals_ar1)[2]
    residuals_ar1[:, i] .= rand.(Normal.(θ_ar1[end] * residuals_ar1[:, i-1], θ_ar1[end-1]))
end
model_ar1 = ebm_wrap(θ_ar1[1:end-2])
model_sim_ar1 = residuals_ar1 .+ model_ar1'
q90_ar1 = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_ar1; dims=1) # compute 90% prediction interval

plot(hind_years, model_discrep, color=ColorSchemes.tol_bright[1], linewidth=3, label="Discrepancy Model", ribbon=(model_discrep .- q90_discrep[1, :], q90_discrep[2, :] .- model_discrep), fillalpha=0.2, xlabel="Year", ylabel="Temperature anomaly (°C)", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)
plot!(hind_years, model_ar1, color=ColorSchemes.tol_bright[2], linewidth=3, label="AR(1) Model", ribbon=(model_ar1 .- q90_ar1[1, :], q90_ar1[2, :] .- model_ar1), fillalpha=0.2)
plot!(hind_years, model_iid, color=ColorSchemes.tol_bright[3], linewidth=3, label="Normal IID Model", ribbon=(model_iid .- q90_iid[1, :], q90_iid[2, :] .- model_iid), fillalpha=0.2)
scatter!(hind_years, temp_obs, color=:black, label="Data")
```

## Projections (RCP 2.6)

```{julia}
#| fig-align: center
#| label: fig-simulation
#| fig-cap: Projections from different EBM fits.

# iid simulations
residuals_iid = rand(Normal(0, θ[end]), (n_samples, length(sim_years)))
model_iid = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p=θ[1:end-1])
model_sim_iid = residuals_iid .+ model_iid'
q90_iid = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval

# ar1 simulations
residuals_ar1 = zeros(n_samples, length(sim_years))
residuals_ar1[:, 1] = rand(Normal(0, θ_ar1[end-1] / sqrt(1-θ_ar1[end]^2)), n_samples)
for i = 2:size(residuals_ar1)[2]
    residuals_ar1[:, i] .= rand.(Normal.(θ_ar1[end] * residuals_ar1[:, i-1], θ_ar1[end-1]))
end
model_ar1 = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p=θ_ar1[1:end-2])
model_sim_ar1 = residuals_ar1 .+ model_ar1'
q90_ar1 = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_ar1; dims=1) # compute 90% prediction interval

# discrepancy simulations
y_err = zeros(length(sim_years))
y_err[1:length(hind_years)] = temp_sd
Σ = ar_covariance_mat(θ_discrep[end-1], θ_discrep[end], zeros(length(sim_years)))
residuals_discrep = rand(MvNormal(zeros(length(sim_years)), Σ), n_samples)
model_discrep = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p=θ_discrep[1:end-2])
model_sim_discrep = (residuals_discrep .+ model_discrep)'
q90_discrep = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_discrep; dims=1) # compute 90% prediction interval

plot(sim_years, model_discrep, color=ColorSchemes.tol_bright[1], linewidth=3, label="Discrepancy Model", ribbon=(model_discrep .- q90_discrep[1, :], q90_discrep[2, :] .- model_discrep), fillalpha=0.2, xlabel="Year", ylabel="Temperature anomaly (°C)", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)
plot!(sim_years, model_ar1, color=ColorSchemes.tol_bright[2], linewidth=3, label="AR(1) Model", ribbon=(model_ar1 .- q90_ar1[1, :], q90_ar1[2, :] .- model_ar1), fillalpha=0.2)
plot!(sim_years, model_iid, color=ColorSchemes.tol_bright[3], linewidth=3, label="Normal IID Model", ribbon=(model_iid .- q90_iid[1, :], q90_iid[2, :] .- model_iid), fillalpha=0.2)
scatter!(hind_years, temp_obs, color=:black, label="Data")
ylims!(-0.4, 2)
```

# Bayesian Statistics

## Prior Information

**So far**: no way to use prior information about parameters (other than bounds on MLE optimization).


## Bayes' Rule

Original version [@Bayes1763-at]:

$$P(A | B) = \frac{P(B | A) \times P(A)}{P(B)} \quad \text{if} \quad P(B) \neq 0.$$

## Bayes' Rule

"Modern" version [@Laplace1774-nf]:

$$p(\theta | y) = \frac{p(y | \theta)}{p(y)} p(\theta)$$

## Bayes' Rule

"Modern" version [@Laplace1774-nf]:

$$\underbrace{{p(\theta | y)}}_{\text{posterior}} = \frac{\overbrace{p(y | \theta)}^{\text{likelihood}}}{\underbrace{p(y)}_\text{normalization}} \overbrace{p(\theta)}^\text{prior}$$

## On The Normalizing Constant

The normalizing constant (also called the **marginal likelihood**) is the integral
$$p(y) = \int_\Theta p(y | \theta) p(\theta) d\theta.$$

Since this *generally* doesn't depend on $\theta$, it can often be ignored, as the relative probabilities don't change. 

## Bayes' Rule (Ignoring Normalizing Constants)

The version of Bayes' rule which matters the most for 95% (approximate) of Bayesian statistics:

$$p(\theta | y) \propto p(y | \theta) \times p(\theta)$$

> "The posterior is the prior times the likelihood..."

## How To Choose A Prior?

**One perspective**: Priors should reflect "actual knowledge" independent of the analysis [@Jaynes2003-lx]

**Another**: Priors are part of the probability model, and can be specified/changed accordingly based on predictive skill [@Gelman2017-zp; @Gelman2013-dw]

## What Makes A Good Prior?

- Reflects level of understanding (informative vs. weakly informative vs. non-informative).
- Does not zero out probability of plausible values.
- Regularization (extreme values should be less probable)

## What Makes A Bad Prior?

- Assigns probability zero to plausible values;
- Weights implausible values equally as more plausible ones;
- Double counts information (*e.g.* fitting a prior to data which is also used in the likelihood)
- Chosen based on vibes.

## A Coin Flipping Example

We would like to understand if a coin-flipping game is fair. We've observed the following sequence of flips:

```{julia}
#| echo: true
flips = ["H", "H", "H", "T", "H", "H", "H", "H", "H"]
```

## Coin Flipping Likelihood

The data-generating process here is straightforward: we can represent a coin flip with a heads-probability of $\theta$ as a sample from a Bernoulli distribution,

$$y_i \sim \text{Bernoulli}(\theta).$$

```{julia}
#| echo: true

flip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== "H"))
θ_mle = Optim.optimize(θ -> -flip_ll(θ), 0, 1).minimizer
round(θ_mle, digits=2)
```

## Coin Flipping Prior

Suppose that we spoke to a friend who knows something about coins, and she tells us that it is extremely difficult to make a passable weighted coin which comes up heads more than 75% of the time.

## Coin Flipping Prior

:::: {.columns}
::: {.column width=50%}
Since $\theta$ is bounded between 0 and 1, we'll use a Beta distribution for our prior, specifically $\text{Beta}(4,4)$.
:::
::: {.column width=50%}
```{julia}
#| echo: true
#| code-fold: true
#| label: fig-beta-prior
#| fig-cap: Beta prior for coin flipping example
#| fig-align: center

prior_dist = Beta(5, 5)
plot(prior_dist; label=false, xlabel=L"$θ$", ylabel=L"$p(θ)$", linewidth=3, tickfontsize=16, guidefontsize=18)
plot!(size=(500, 500))
```
:::
::::

## *Maximum A Posteriori* Estimate

Combining using Bayes' rule lets us calculate the **maximum *a posteriori* (MAP)** estimate: 

```{julia}
#| echo: true
#| code-line-numbers: "|2,3"

flip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== "H"))
flip_lprior(θ) = logpdf(Beta(5, 5), θ)
flip_lposterior(θ) = flip_ll(θ) + flip_lprior(θ)
θ_map = Optim.optimize(θ -> -(flip_lposterior(θ)), 0, 1).minimizer
round(θ_map, digits=2)
```

## Coin Flipping Posterior Distribution

```{julia}
#| echo: true
#| code-fold: true
#| label: fig-code-posterior
#| fig-cap: Posterior distribution for the coin-flipping example

θ_range = 0:0.01:1
plot(θ_range, flip_lposterior.(θ_range), color=:black, label="Posterior", linewidth=3, tickfontsize=16, legendfontsize=16, guidefontsize=18, bottom_margin=5mm, left_margin=5mm)
vline!([θ_map], color=:red, label="MAP", linewidth=2)
vline!([θ_mle], color=:blue, label="MLE", linewidth=2)
xlabel!(L"$\theta$")
ylabel!("Posterior Density")
plot!(size=(1000, 450))
```

## Bayes and Parametric Uncertainty

**Frequentist**: Parametric uncertainty is purely the result of *sampling variability*

**Bayesian**: Parameters have probabilities based on *consistency with data and priors*.

## Bayesian Updating

- The posterior is a "compromise" between the prior and the data.
- The posterior mean is a weighted combination of the data and the prior mean.
- The weights depend on the prior and the likelihood variances.
- More data *usually* makes the posterior more confident.

# Example: Local Sea Level Extremes

## San Francisco Tide Gauge Data

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-data
#| fig-cap: Annual maxima surge data from the San Francisco, CA tide gauge.

# read in data and get annual maxima
function load_data(fname)
    date_format = DateFormat("yyyy-mm-dd HH:MM:SS")
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = @chain fname begin
        CSV.read(DataFrame; header=false)
        rename("Column1" => "year", "Column2" => "month", "Column3" => "day", "Column4" => "hour", "Column5" => "gauge")
        # need to reformat the decimal date in the data file
        @transform :datetime = DateTime.(:year, :month, :day, :hour)
        # replace -99999 with missing
        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)
        select(:datetime, :gauge)
    end
    return df
end

dat = load_data("data/surge/h551.csv")

# detrend the data to remove the effects of sea-level rise and seasonal dynamics
ma_length = 366
ma_offset = Int(floor(ma_length/2))
moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]
dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))

# group data by year and compute the annual maxima
dat_ma = dropmissing(dat_ma) # drop missing data
dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime => x->year.(x)), :datetime_function))
delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet
rename!(dat_annmax, :datetime_function => :Year)
select!(dat_annmax, [:Year, :residual])
dat_annmax.residual = dat_annmax.residual / 1000 # convert to m

# make plots
p1 = plot(
    dat_annmax.Year,
    dat_annmax.residual;
    xlabel="Year",
    ylabel="Annual Max Tide Level (m)",
    label=false,
    marker=:circle,
    markersize=5,
    tickfontsize=16,
    guidefontsize=18
)
p2 = histogram(
    dat_annmax.residual,
    normalize=:pdf,
    orientation=:horizontal,
    label=:false,
    xlabel="PDF",
    ylabel="",
    yticks=[],
    tickfontsize=16,
    guidefontsize=18
)

l = @layout [a{0.7w} b{0.3w}]
plot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)
plot!(size=(1000, 450))
```

## Proposed Probability Model

$$
\begin{align*}
& y \sim LogNormal(\mu, \sigma) \tag{likelihood}\\
& \left. \begin{aligned} 
& \mu \sim Normal(0, 1) \\
& \sigma \sim HalfNormal(0, 5)
\end{aligned} \right\} \tag{priors}
\end{align*}
$$

Want to find:

$$p(\mu, \sigma | y) \propto p(y | \mu, \sigma) p(\mu)p(\sigma)$$

## Are Our Priors Reasonable?

Hard to tell! Let's simulate data to see we get plausible outcomes.

This is called a **prior predictive check**.

## Prior Predictive Check

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-cap: Prior predictive check of return periods with revised model
#| label: fig-surge-prior-1

# sample from priors
μ_sample = rand(Normal(0, 1), 1_000)
σ_sample = rand(truncated(Normal(0, 5), 0, +Inf), 1_000)

# define return periods and cmopute return levels for parameters
return_periods = 2:100
return_levels = zeros(1_000, length(return_periods))
for i in 1:1_000
    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))
end

plt_prior_1 = plot(; yscale=:log10, yticks=10 .^ collect(0:2:16), ylabel="Return Level (m)", xlabel="Return Period (yrs)",
    tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)
for idx in 1:1_000
    label = idx == 1 ? "Prior" : false
    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)
end
plt_prior_1
```

## Let's Revise the Prior

$$
\begin{align*}
& y \sim LogNormal(\mu, \sigma) \tag{likelihood}\\
& \left. \begin{aligned} 
& \mu \sim Normal(0, 0.5) \\
& \sigma \sim HalfNormal(0, 0.1)
\end{aligned} \right\} \tag{priors}
\end{align*}
$$



## Prior Predictive Check 2


```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-cap: Prior predictive check of return periods with revised model
#| label: fig-surge-prior-2

# sample from priors
μ_sample = rand(Normal(0, 0.5), 1_000)
σ_sample = rand(truncated(Normal(0, 0.1), 0, +Inf), 1_000)

return_periods = 2:100
return_levels = zeros(1_000, length(return_periods))
for i in 1:1_000
    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))
end

plt_prior_2 = plot(; ylabel="Return Level (m)", xlabel="Return Period (yrs)", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm)
for idx in 1:1_000
    label = idx == 1 ? "Prior" : false
    plot!(plt_prior_2, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)
end
plt_prior_2
```

## Compute Posterior

:::: {.columns}
::: {.column width=70%}
```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-posterior
#| fig-cap: Posterior samples from surge model.

ll(μ, σ) = sum(logpdf(LogNormal(μ, σ), dat_annmax.residual))
lprior(μ, σ) = logpdf(Normal(0, 0.5), μ) + logpdf(truncated(Normal(0, 0.1), 0, Inf), σ)
lposterior(μ, σ) = ll(μ, σ) + lprior(μ, σ)

p_map = optimize(p -> -lposterior(p[1], p[2]), [0.0, 0.0], [1.0, 1.0], [0.5, 0.5]).minimizer

μ = 0.15:0.005:0.35
σ = 0.04:0.01:0.1
posterior_vals = @. lposterior(μ', σ)

contour(μ, σ, posterior_vals, 
    levels=100, 
    clabels=false, 
    cbar=true, lw=1, 
    fill=(true,cgrad(:grays,[0,0.1,1.0])),
    tickfontsize=16,
    guidefontsize=18,
    legendfontsize=18,
    right_margin=20mm,
    bottom_margin=5mm,
    left_margin=5mm
)
scatter!([p_map[1]], [p_map[2]], label="MAP", markersize=10, marker=:star)
xlabel!(L"$\mu$")
ylabel!(L"$\sigma$")
plot!(size=(900, 400))
```
:::
::: {.column width=30%}
```{julia}
#| echo: false
#| output: true

p_map
```
:::
::::

## Assess MAP Fit

```{julia}
#| label: fig-surge-fit
#| output: true
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| fig-align: center
#| fig-cap: Checks for model fit.


p1 = histogram(
    dat_annmax.residual,
    normalize=:pdf,
    legend=:false,
    ylabel="PDF",
    xlabel="Annual Max Tide Level (m)",
    tickfontsize=16,
    guidefontsize=18,
    bottom_margin=5mm, left_margin=5mm
)
plot!(p1, LogNormal(p_map[1], p_map[2]),
    linewidth=3,
    color=:red)
xlims!(p1, (1, 1.7))
plot!(p1, size=(600, 450))

return_periods = 2:500
return_levels = quantile.(LogNormal(p_map[1], p_map[2]), 1 .- (1 ./ return_periods))

# function to calculate exceedance probability and plot positions based on data quantile
function exceedance_plot_pos(y)
    N = length(y)
    ys = sort(y; rev=false) # sorted values of y
    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability
    xp = 1 .- nxp
    return xp, ys
end
xp, ys = exceedance_plot_pos(dat_annmax.residual)

p2 = plot(return_periods, return_levels, linewidth=3, color=:blue, label="Model Fit", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)
scatter!(p2, 1 ./ xp, ys, label="Observations", color=:black, markersize=5)
xlabel!(p2, "Return Period (yrs)")
ylabel!(p2, "Return Level (m)")
xlims!(-1, 300)
plot!(p2, size=(600, 450))

display(p1)
display(p2)
```

## What About The Posterior Distribution?

One of the points of Bayesian statistics is we get a distribution over parameters.

## Conjugate Priors

When the mathematical forms of the likelihood and the prior(s) are **conjugate**, the posterior is a nice closed-form distribution.

**Examples**:

* Normal $p(y | \mu)$, Normal $p(\mu)$ &Rightarrow; Normal $p(\mu | y)$
* Binomial $p(y | \theta)$, Beta $(p\theta)$, &Rightarrow; Beta $p(\theta | y)$

## But In General...

Conjugate priors are often convenient, but may be poor choices.

We will talk about how to sample more generally with Monte Carlo later.

# Key Points and Upcoming Schedule

## Key Points: Discrepancy

- Modeling discrepancy can separate system state-relevant errors from observation errors.
- Further decomposition of data generating process.
- **When hindcasting, include observation errors; do not when projecting!**

## Key Points: Bayesian Statistics

- Bayesian probability: parameters have probabilities conditional on data
- Need to specify prior distribution (think generatively!).
- Be transparent and principled about prior choices (sensitivity analyses?).
- Maximum *a posteriori* gives "most probable" parameter values
- Will talk more about general sampling later.

## Next Class(es)

**Monday**: Extreme Value Theory and Models

**Wednesday**: No class! Develop project proposals.

## Assessments

**Friday**: Exercise 1 due by 9pm.

HW2 available! Due 2/23 by 9pm.

# References

## References