{
  "hash": "f41a26fb416b7c7f86568cfc28383081",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Information Criteria\"\nsubtitle: \"Lecture 19\"\nauthor: \"Vivek Srikrishnan\"\ncourse: \"BEE 4850\"\ninstitution: \"Cornell University\"\ndate: \"April 22, 2024\"\nformat:\n    revealjs:\n        slide-number: c/t\n        show-slide-number: all\n        center-title-slide: true\n        width: 1280\n        height: 720\n        transition: none\n        toc: true\n        toc-depth: 1\n        toc-title: \"Overview\"\n        history: false\n        link-external-newwindow: true\n        theme: ../sass/slides.scss\n        template-partials:\n            - title-slide.html\n        menu:\n            numbers: true\n        html-math-method: \n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js\"\n        include-in-header: mathjax-config.html\n        date-format: long\n        email-obfuscation: javascript\n        chalkboard:\n            theme: whiteboard\n            buttons: true\n        touch: false\n        controls: true\nengine: julia\njulia:\n    exeflags: [\"+1.10.4\"]          \nexecute:\n    freeze: auto\n---\n\n\n\n\n\n\n\n# Review of Last Class\n\n## Bias-Variance Tradeoff\n\n\n**Key Idea**: Model selection consists of navigating the bias-variance tradeoff.\n\nModel error (*e.g.* RMSE) is a combination of *irreducible error*, *bias*, and *variance*.\n\n## Bias\n\n**Bias** is error from mismatches between the model predictions and the data ($\\text{Bias}[\\hat{f}] = \\mathbb{E}[\\hat{f}] - y$).\n\nBias comes from under-fitting meaningful relationships between inputs and outputs.\n\n- too few degrees of freedom (\"too simple\")\n- neglected processes.\n\n## Variance\n\n**Variance** is error from over-sensitivity to small fluctuations in inputs ($\\text{Variance} = \\text{Var}(\\hat{f})$).\n\nVariance can come from over-fitting noise in the data.\n\n- too many degrees of freedom (\"too complex\")\n- poor identifiability\n\n## Bias-Variance Tradeoff\n\n**Upshot**: For achieve a fixed error level, you can reduce bias (more \"complex\" model) or you can reduce variance (more \"simple\" model) but there's a tradeoff.\n\n## Bias-Variance Tradeoff More Generally \n\nThis decomposition is for MSE, but the **principle holds more generally**.\n\n- Models which perform better \"on average\" over the training data (low bias) are more likely to overfit (high variance);\n- Models which have less uncertainty for training data (low variance) will do worse \"on average\".\n\n## Cross-Validation\n\nThe \"gold standard\" way to test for predictive performance is **cross-validation**:\n\n1. Split data into training/testing sets;\n2. Calibrate model to training set;\n3. Check for predictive ability on testing set.\n\n## Leave-One-Out Cross-Validation\n\n1. Drop one value $y_i$.\n2. Refit model on rest of data $y_{-i}$.\n3. Evaluate $\\log p(y_i | y_{-i})$.\n4. Repeat on rest of data set.\n\n\n## Leave-$k$-Out Cross-Validation\n\nDrop $k$ values, refit model on rest of data, check for predictive skill.\n\nAs $k \\to n$, this reduces to the prior predictive distribution\n$$p(y^{\\text{rep}}) = \\int_{\\theta} p(y^{\\text{rep}} | \\theta) p(\\theta) d\\theta.$$\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nThe out-of-sample predictive fit of a new data point $\\tilde{y}_i$ is\n\n$$\n\\begin{align}\n\\log p_\\text{post}(\\tilde{y}_i) &= \\log \\mathbb{E}_\\text{post}\\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\log \\int p(\\tilde{y_i} | \\theta) p_\\text{post}(\\theta)\\,d\\theta.\n\\end{align}\n$$\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nHowever, the out-of-sample data $\\tilde{y}_i$ is itself unknown, so we need to compute the *expected out-of-sample log-predictive density*\n\n$$\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p_\\text{post}(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p_\\text{post}(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n$$\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nWhat is the challenge?\n\n::: {.fragment .fade-in}\nWe don't know $P$ (the distribution of new data)!\n\nWe need some measure of the error induced by using an approximating distribution $Q$ from some model.\n:::\n\n# Information Criteria\n\n## Information Criteria\n\n\"Information criteria\" refers to a category of estimators of prediction error.\n\nThe idea: estimate predictive error using the fitted model.\n\n## Information Criteria Overview\n\nThere is a common framework for all of these:\n\nIf we compute the expected log-predictive density for the existing data $p(y | \\theta)$, this will be too good of a fit and will overestimate the predictive skill for new data.\n\n## Information Criteria Corrections\n\nWe can adjust for that bias by correcting for the *effective number of parameters*, which can be thought of as the expected degrees of freedom in a model contributing to overfitting.\n\n\n\n## Akaike Information Criterion (AIC)\n\nThe \"first\" information criterion that most people see.\n\nUses a point estimate (the maximum-likelihood estimate $\\hat{\\theta}_\\text{MLE}$) to compute the log-predictive density for the data, corrected by the number of parameters $k$:\n\n$$\\widehat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{MLE}) - k.$$\n\n## AIC Formula\n\nThe AIC is defined as $-2\\widehat{\\text{elpd}}_\\text{AIC}$.\n\nDue to this convention, lower AICs are better (they correspond to a higher predictive skill).\n\n## AIC Correction Term\n\nIn the case of a normal model with independent and identically-distributed data and uniform priors, $k$ is the asymptotically \"correct\" bias term (there are modified corrections for small sample sizes).\n\nHowever, with more informative priors and/or hierarchical models, the bias correction $k$ is no longer appropriate, as there is less \"freedom\" associated with each parameter.\n\n## AIC: Storm Surge Example\n\n\n\n\nModels:\n\n1. Stationary (\"null\") model, $y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);$\n2. Time nonstationary (\"null-ish\") model, $y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);$\n3. PDO nonstationary model, $y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)$\n\n## AIC Example\n\n::: {#8 .cell execution_count=1}\n``` {.julia .cell-code code-fold=\"true\"}\nstat_mle = [1258.71, 56.27, 0.017]\nstat_ll = -707.67\nnonstat_mle = [1231.58, 0.42, 52.07, 0.075]\nnonstat_ll = -702.45\npdo_mle = [1255.87, -12.39, 54.73, 0.033]\npdo_ll = -705.24\n\n# compute AIC values\nstat_aic = stat_ll - 3\nnonstat_aic = nonstat_ll - 4\npdo_aic = pdo_ll - 4\n\nmodel_aic = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], LogLik=trunc.(Int64, round.([stat_ll, nonstat_ll, pdo_ll]; digits=0)), AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)))\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div><div style = \"float: left;\"><span>3Ã—3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Model</th><th style = \"text-align: left;\">LogLik</th><th style = \"text-align: left;\">AIC</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Stationary</td><td style = \"text-align: right;\">-708</td><td style = \"text-align: right;\">1421</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Time</td><td style = \"text-align: right;\">-702</td><td style = \"text-align: right;\">1413</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">PDO</td><td style = \"text-align: right;\">-705</td><td style = \"text-align: right;\">1418</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n## AIC Interpretation\n\nAbsolute AIC values have **no meaning**, only the differences $\\Delta_i = \\text{AIC}_i - \\text{AIC}_\\text{min}$.\n\nSome basic rules of thumb (from @Burnham2004-do):\n\n- $\\Delta_i < 2$ means the model has \"strong\" support across $\\mathcal{M}$;\n- $4 < \\Delta_i < 7$ suggests \"less\" support;\n- $\\Delta_i > 10$ suggests \"weak\" or \"no\" support.\n\n## AIC and Model Evidence\n\n$\\exp(-\\Delta_i/2)$ can be thought of as a measure of the likelihood of the model given the data $y$. \n\nThe ratio $$\\exp(-\\Delta_i/2) / \\exp(-\\Delta_j/2)$$ can approximate the relative evidence for  $M_i$ versus $M_j$.\n\n## AIC and Model Averaging\n\nThis gives rise to the idea of *Akaike weights*:\n$$w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.$$\n\nModel projections can then be weighted based on $w_i$, which can be interpreted as the probability that $M_i$ is the best (in the sense of approximating the \"true\" predictive distribution) model in $\\mathcal{M}$.\n\n## Model Averaging vs. Selection\n\nModel averaging can sometimes be beneficial vs. model selection.\n\nModel selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence).\n\n# Key Takeaways and Upcoming Schedule\n\n## Key Takeaways\n\n- LOO-CV is ideal for navigating bias-variance tradeoff but can be computationally prohibitive.\n- Information Criteria are an approximation to LOO-CV based on \"correcting\" for model complexity.\n- Approximation to out of sample predictive error as a penalty for *potential to overfit*.\n\n\n## Next Classes\n\n**Wednesday**: Other Information Criteria\n\n# References\n\n## References\n\n",
    "supporting": [
      "lecture13-1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}