{
  "hash": "f39159ad3c59dbc0c2aa07d88d422e78",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Other Information Criteria\"\nsubtitle: \"Lecture 20\"\nauthor: \"Vivek Srikrishnan\"\ncourse: \"BEE 4850\"\ninstitution: \"Cornell University\"\ndate: \"April 24, 2024\"\nformat:\n    revealjs:\n        slide-number: c/t\n        show-slide-number: all\n        center-title-slide: true\n        width: 1280\n        height: 720\n        transition: none\n        toc: true\n        toc-depth: 1\n        toc-title: \"Overview\"\n        history: false\n        link-external-newwindow: true\n        theme: ../sass/slides.scss\n        template-partials:\n            - title-slide.html\n        menu:\n            numbers: true\n        html-math-method: \n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js\"\n        include-in-header: mathjax-config.html\n        date-format: long\n        email-obfuscation: javascript\n        chalkboard:\n            theme: whiteboard\n            buttons: true\n        touch: false\n        controls: true\nengine: julia\njulia:\n    exeflags: [\"+1.10.4\"]          \nexecute:\n    freeze: auto\n---\n\n\n\n\n\n\n\n# Review of Last Class\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nThe out-of-sample predictive fit of a new data point $\\tilde{y}_i$ is\n\n$$\n\\begin{align}\n\\log p_\\text{post}(\\tilde{y}_i) &= \\log \\mathbb{E}_\\text{post}\\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\log \\int p(\\tilde{y_i} | \\theta) p_\\text{post}(\\theta)\\,d\\theta.\n\\end{align}\n$$\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nHowever, the out-of-sample data $\\tilde{y}_i$ is itself unknown, so we need to compute the *expected out-of-sample log-predictive density*\n\n$$\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p_\\text{post}(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p_\\text{post}(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n$$\n\n## Expected Out-Of-Sample Predictive Accuracy\n\nWhat is the challenge?\n\n::: {.fragment .fade-in}\nWe don't know $P$ (the distribution of new data)!\n\nWe need some measure of the error induced by using an approximating distribution $Q$ from some model.\n:::\n\n## Information Criteria Overview\n\nThere is a common framework for all of these:\n\nIf we compute the expected log-predictive density for the existing data $p(y | \\theta)$, this will be too good of a fit and will overestimate the predictive skill for new data.\n\n## Information Criteria Corrections\n\nWe can adjust for that bias by correcting for the *effective number of parameters*, which can be thought of as the expected degrees of freedom in a model contributing to overfitting.\n\n\n## Akaike Information Criterion (AIC)\n\nThe \"first\" information criterion that most people see.\n\nUses a point estimate (the maximum-likelihood estimate $\\hat{\\theta}_\\text{MLE}$) to compute the log-predictive density for the data, corrected by the number of parameters $k$:\n\n$$\\widehat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{MLE}) - k.$$\n\n## AIC Formula\n\nThe AIC is defined as $-2\\widehat{\\text{elpd}}_\\text{AIC}$.\n\nDue to this convention, lower AICs are better (they correspond to a higher predictive skill).\n\n## AIC Correction Term\n\nIn the case of a normal model with independent and identically-distributed data and uniform priors, $k$ is the asymptotically \"correct\" bias term (there are modified corrections for small sample sizes).\n\nHowever, with more informative priors and/or hierarchical models, the bias correction $k$ is no longer appropriate, as there is less \"freedom\" associated with each parameter.\n\n\n\n\n\n\n\n\n## AIC Interpretation\n\nAbsolute AIC values have **no meaning**, only the differences $\\Delta_i = \\text{AIC}_i - \\text{AIC}_\\text{min}$.\n\nSome basic rules of thumb (from @Burnham2004-do):\n\n- $\\Delta_i < 2$ means the model has \"strong\" support across $\\mathcal{M}$;\n- $4 < \\Delta_i < 7$ suggests \"less\" support;\n- $\\Delta_i > 10$ suggests \"weak\" or \"no\" support.\n\n## AIC and Model Evidence\n\n$\\exp(-\\Delta_i/2)$ can be thought of as a measure of the likelihood of the model given the data $y$. \n\nThe ratio $$\\exp(-\\Delta_i/2) / \\exp(-\\Delta_j/2)$$ can approximate the relative evidence for  $M_i$ versus $M_j$.\n\n## AIC and Model Averaging\n\nThis gives rise to the idea of *Akaike weights*:\n$$w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.$$\n\nModel projections can then be weighted based on $w_i$, which can be interpreted as the probability that $M_i$ is the best (in the sense of approximating the \"true\" predictive distribution) model in $\\mathcal{M}$.\n\n## Model Averaging vs. Selection\n\nModel averaging can sometimes be beneficial vs. model selection.\n\nModel selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence).\n\n# Other Informtion Criteria\n\n## Deviance Information Criterion (DIC)\n\nThe Deviance Information Criterion (DIC) is a more Bayesian generalization of AIC which uses the posterior mean \n$$\\hat{\\theta}_\\text{Bayes} = \\mathbb{E}\\left[\\theta | y\\right]$$\nand a bias correction derived from the data.\n\n## DIC\n\n$$\\widehat{\\text{elpd}}_\\text{DIC} = \\log p(y | \\hat{\\theta}_\\text{Bayes}) - p_{\\text{DIC}},$$\nwhere\n$$p_\\text{DIC} = 2\\left(\\log p(y | \\hat{\\theta}_\\text{Bayes}) - \\mathbb{E}_\\text{post}\\left[\\log p(y | \\theta)\\right]\\right).$$\n\nThen, as with AIC, $$\\text{DIC} = -2\\widehat{\\text{elpd}}_\\text{DIC}.$$\n\n##  DIC: Effective Number of Parameters\n\n**What is the meaning of $p_\\text{DIC}$?**\n\n- The difference between the average log-likelihood (across parameters) and the log-likelihood at a parameter average measures \"degrees of freedom\".\n- The DIC adjustment assumes independence of residuals for fixed $\\theta$.\n\n## AIC vs. DIC\n\nAIC and DIC often give similar results, but don't have to. \n\nThe key difference is the impact of priors on parameter estimation and model degrees of freedom.\n\n## AIC vs. DIC Storm Surge Example\n\nModels:\n\n1. Stationary (\"null\") model, $y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);$\n2. Time nonstationary (\"null-ish\") model, $y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);$\n3. PDO nonstationary model, $y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)$\n\n## AIC vs. DIC\n\n::: {#10 .cell execution_count=1}\n``` {.julia .cell-code code-fold=\"true\"}\n# fit models\nstat_chain = sample(stat_mod, NUTS(), MCMCThreads(), 10_000, 4)\nnonstat_chain = sample(nonstat_mod, NUTS(), MCMCThreads(), 10_000, 4)\npdo_chain = sample(pdo_mod, NUTS(), MCMCThreads(), 10_000, 4)\n\nstat_est = mean(stat_chain)[:, 2]\nnonstat_est = mean(nonstat_chain)[:, 2]\npdo_est = mean(pdo_chain)[:, 2]\n\nstat_bayes = loglikelihood(stat_mod, (μ = stat_est[1], σ = stat_est[2], ξ = stat_est[3]))\nnonstat_bayes = loglikelihood(nonstat_mod, (a = nonstat_est[1], b = nonstat_est[2], σ = nonstat_est[3], ξ = nonstat_est[4]))\npdo_bayes = loglikelihood(pdo_mod, (a = pdo_est[1], b = pdo_est[2], σ = pdo_est[3], ξ = pdo_est[4]))\n\nstat_ll = mean([loglikelihood(stat_mod, row) for row in eachrow(DataFrame(stat_chain))]) \nnonstat_ll = mean([loglikelihood(nonstat_mod, row) for row in eachrow(DataFrame(nonstat_chain))])\npdo_ll = mean([loglikelihood(pdo_mod, row) for row in eachrow(DataFrame(pdo_chain))])\n\nstat_dic = 2 * stat_ll - stat_bayes\nnonstat_dic = 2 * nonstat_ll - nonstat_bayes\npdo_dic = 2 * pdo_ll - pdo_bayes\n\nmodel_dic = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)), DIC=trunc.(Int64, round.(-2 * [stat_dic, nonstat_dic, pdo_dic]; digits=0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-yellow-fg ansi-bold\">┌ </span><span class=\"ansi-yellow-fg ansi-bold\">Warning: </span>Only a single thread available: MCMC chains are not sampled in parallel\n<span class=\"ansi-yellow-fg ansi-bold\">└ </span><span class=\"ansi-bright-black-fg\">@ AbstractMCMC ~/.julia/packages/AbstractMCMC/kwj9g/src/sample.jl:384</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)   0%|                               |  ETA: N/A</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.0015625\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.05\n<span class=\"ansi-green-fg\">Sampling (1 thread)  25%|███████▊                       |  ETA: 0:00:28</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)  50%|███████████████▌               |  ETA: 0:00:11</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.00625\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.0125\n<span class=\"ansi-green-fg\">Sampling (1 thread)  75%|███████████████████████▎       |  ETA: 0:00:04</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:13</span>\n<span class=\"ansi-bright-black-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:13</span>\n<span class=\"ansi-yellow-fg ansi-bold\">┌ </span><span class=\"ansi-yellow-fg ansi-bold\">Warning: </span>Only a single thread available: MCMC chains are not sampled in parallel\n<span class=\"ansi-yellow-fg ansi-bold\">└ </span><span class=\"ansi-bright-black-fg\">@ AbstractMCMC ~/.julia/packages/AbstractMCMC/kwj9g/src/sample.jl:384</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)   0%|                               |  ETA: N/A</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.00625\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.0125\n<span class=\"ansi-green-fg\">Sampling (1 thread)  25%|███████▊                       |  ETA: 0:00:18</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)  50%|███████████████▌               |  ETA: 0:00:08</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.05\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.05\n<span class=\"ansi-green-fg\">Sampling (1 thread)  75%|███████████████████████▎       |  ETA: 0:00:03</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:12</span>\n<span class=\"ansi-bright-black-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:12</span>\n<span class=\"ansi-yellow-fg ansi-bold\">┌ </span><span class=\"ansi-yellow-fg ansi-bold\">Warning: </span>Only a single thread available: MCMC chains are not sampled in parallel\n<span class=\"ansi-yellow-fg ansi-bold\">└ </span><span class=\"ansi-bright-black-fg\">@ AbstractMCMC ~/.julia/packages/AbstractMCMC/kwj9g/src/sample.jl:384</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)   0%|                               |  ETA: N/A</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.025\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.2\n<span class=\"ansi-green-fg\">Sampling (1 thread)  25%|███████▊                       |  ETA: 0:00:12</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread)  50%|███████████████▌               |  ETA: 0:00:06</span>\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.2\n<span class=\"ansi-cyan-fg ansi-bold\">┌ </span><span class=\"ansi-cyan-fg ansi-bold\">Info: </span>Found initial step size\n<span class=\"ansi-cyan-fg ansi-bold\">└ </span>  ϵ = 0.2\n<span class=\"ansi-green-fg\">Sampling (1 thread)  75%|███████████████████████▎       |  ETA: 0:00:03</span>\n<span class=\"ansi-green-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:09</span>\n<span class=\"ansi-bright-black-fg\">Sampling (1 thread) 100%|███████████████████████████████| Time: 0:00:09</span>\n</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div><div style = \"float: left;\"><span>3×3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Model</th><th style = \"text-align: left;\">AIC</th><th style = \"text-align: left;\">DIC</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Stationary</td><td style = \"text-align: right;\">1421</td><td style = \"text-align: right;\">1421</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Time</td><td style = \"text-align: right;\">1413</td><td style = \"text-align: right;\">1413</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">PDO</td><td style = \"text-align: right;\">1418</td><td style = \"text-align: right;\">1419</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n\n## Convergence of AIC and DIC\n\nBoth AIC and DIC converge (as $n \\to \\infty$) to expected leave-one-out CV.\n\nThe question is how well they do under limited sample sizes!\n\n## Watanabe-Akaike Information Criterion (WAIC)\n\n$$\\widehat{\\text{elpd}}_\\text{WAIC} = \\sum_{i=1}^n \\log \\int p(y_i | \\theta) p_\\text{post}(\\theta)\\,d\\theta - p_{\\text{WAIC}},$$\n\nwhere\n$$p_\\text{WAIC} = \\sum_{i=1}^n \\text{Var}_\\text{post}\\left(\\log p(y_i | \\theta)\\right).$$\n\n## WAIC Correction Factor\n\n$p_\\text{WAIC}$ is an estimate of the number of \"unconstrained\" parameters in the model.\n\n- A parameter counts as 1 if its estimate is \"independent\" of the prior;\n- A parameter counts as 0 if it is fully constrained by the prior.\n- A parameter gives a partial value if both the data and prior are informative.\n\n## WAIC vs. AIC and DIC\n\n- WAIC can be viewed as an approximation to leave-one-out CV, and averages over the entire posterior, vs. AIC and DIC which use point estimates.\n- But it doesn't work well with highly structured data; no real alternative to more clever uses of Bayesian cross-validation.\n\n## \"Bayesian\" \"Information\" Criterion (BIC)\n\n$$\\text{BIC} = -2\\log p(y | \\hat{\\theta}_\\text{MLE}) + k\\log n.$$\n\nBIC:\n\n- Is not Bayesian (it relies on the MLE);\n- Has no relationship to information theory (unlike AIC/DIC);\n- Assumes $\\mathcal{M}$-closed (e.g. that the true model is under consideration).\n\n## BIC\n\nBIC approximates the *prior* log-predictive likelihood and leave-$k$-out cross-validation (hence the extra penalization for additional parameters).\n\n**This is why it's odd when model selection consists of examining both AIC and BIC: these are different quantities with different purposes!**\n\n\n# Key Takeaways and Upcoming Schedule\n\n## Key Takeaways\n\n- Information Criteria are an approximation to LOO-CV based on \"correcting\" for model complexity.\n- AIC and DIC can be used to approximate LOO-CV.\n- BIC is an entirely different measure, approximating the prior predictive distribution (leave-$k$-out CV).\n\n## An Important Caveat\n\n**Model selection can result in significant overfitting when separated from hypothesis-driven model development** [@Freedman1983-xq; @Smith2018-wt]\n\n## An Important Caveat\n\n- Better off thinking about the scientific or engineering problem you want to solve and use domain knowledge/checks rather than throwing a large number of possible models into the selection machinery.\n- Regularizing priors reduce potential for overfitting.\n- Model averaging [@Hoeting2021-vx] and stacking [@Yao2018-rr] can combine multiple models as an alternative to selection.\n\n\n## Next Classes\n\n**Next Week**: Emulating Complex Models\n\n# References\n\n## References\n\n",
    "supporting": [
      "lecture13-2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}